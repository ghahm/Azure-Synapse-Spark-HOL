# 1. Dataframe 생성하고 자료 조회하기
baseDF = spark.read.load('abfss://demodata@adlsgen2krc.dfs.core.windows.net/sparkhol/wordcount-data/Harry-Potter-and-the-Sorcerer.txt', format='text')
display(baseDF.limit(10))

baseDF.take(1) # 첫번째 행 return
#baseDF.collect() # 전체 데이터 return

# 2. Transformations 함수 예: map() vs. flatMap  아래 2개의 take() 결과 비교
# map() : Return a new RDD by applying a function to each element of this RDD
splitDF01 = baseDF.map(lambda line: line.split(" "))
splitDF01.take(5)

# flatMap() : Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results
splitDF02 = baseDF.flatMap(lambda line: line.split(" "))
splitDF02.take(5)

# 3. splitDF02의 각 Key에 Integer 값을 assign (Word count를 위한 사전 작업)
mappedDF = splitDF02.map(lambda line: (line,1))
mappedDF.take(5)

# 4. mappedDF의 각 Key 별로 aggregation을 수행하여 각 단어의 개수 계산하기
# reduceByKey() : Merge the values for each key using an associative reduce function
reducedDF = mappedDF.reduceByKey(lambda a,b: a+b) # 각 Key의 Integer 값을 합산
#reducedDF.take(20)

reducedDF.collect()

# 5. reducedDF를 2번째 컬럼을 기준으로 내림차순 정렬하여 Top 10 Word Count를 조회
# sortBy(keyfunc, ascending=True, numPartitions=None) : Sorts this RDD by the given keyfunc
reducedDF.sortBy(lambda x: x[1], False).take(10)
